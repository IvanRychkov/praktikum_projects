# Выявление токсичных комментариев

**Статус:** Завершён.

**Цель проекта:** Обучить модель классификации, которая выявит среди текстов токсичные и отправит их на модерацию.

**Результаты работы:**
   - Мы выполнили очистку данных и разделили их на обучающую и тестовую выборки.
   - Применили три подхода к векторизации текстов в датасете:
       - Bag of words,
       - TF-IDF,
       - BERT-эмбеддинги.
- Испытали лемматизацию `WordNet` и стемминг `SnowballStemmer` из пакета `NLTK`. Оба варианта в целом снизили качество предсказаний, поэтому мы от них отказались.
- Выяснили опытным путём, что восклицательные знаки в текстах **не свидетельствуют** об их токсичности.
- Всего 5000 объектов, закодированных в BERT-эмбеддинги, дали нам среднюю F1 в 0.65, что очень перспективно. В целом это очень ресурсоёмкая операция, поэтому мы не стали получать эмбеддинги для всех объектов.
- Мы дополнительно обучили модель `CatBoost`, но качество ответов оказалось недостаточным и не оправдало затраченное время.
- Также мы испытали Наивного Байеса на BOW и отметили высочайшую скорость обучения, хотя качество не было самым лучшим, около 0.66.
- Bag of words дал наилучшее значение метрики F1, поэтому мы взяли его за основу и на нём оптимизировали гиперпараметры логистической регрессии. Итоговым значением F1 стало *0.769*.

---

**Стек технологий**: *pandas, NumPy, NLTK, PyTorch, transformers, Matplotlib, Seaborn, scikit-learn, CatBoost, LightGBM, Joblib*. Также использовал собственные [функции-помощники](https://github.com/IvanRychkov/toads).
